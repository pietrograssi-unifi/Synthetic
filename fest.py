"""
This script performs a comparative assessment of synthetic datasets generated by 
various architectures (CTGAN, TVAE, TTVAE) against the ground truth (BaseDataset).

Methodology:
The framework evaluates data utility and privacy preservation using a dual-metric approach:
1. Statistical Fidelity: Kolmogorov-Smirnov (KS) Test and Correlation Matrix Distance.
2. Privacy Preservation: Distance to Closest Record (DCR) and Adversarial Accuracy.

Robustness Note:
The script automatically aligns feature spaces between real and synthetic data, 
ensuring stability even in the presence of schema mismatches.

Author: Pietro Grassi
Date: January 2026
Dependencies: pandas, numpy, scipy, sklearn, seaborn, matplotlib
"""

import os
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ks_2samp
from scipy.spatial.distance import cdist
from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.neighbors import NearestNeighbors

# Suppress warnings for cleaner console output
warnings.filterwarnings("ignore")

# ==============================================================================
# SECTION A: CORE METRIC CALCULATORS
# ==============================================================================

class BaseCalculator:
    """
    Base class for all FEST metric calculators.
    
    Responsibility:
    - Aligns the feature space between Real and Synthetic datasets.
    - Handles missing values (imputation) to ensure metric stability.
    - Identifies numerical and categorical features automatically.
    """
    def __init__(self, real_data, synthetic_data, real_name="Real", synthetic_name="Synthetic"):
        # 1. Feature Alignment (Intersection Logic)
        # Only evaluate columns present in BOTH datasets to prevent KeyErrors
        self.common_cols = real_data.columns.intersection(synthetic_data.columns)
        
        if len(self.common_cols) < len(real_data.columns):
            missing = set(real_data.columns) - set(self.common_cols)
            # print(f"   [Info] {len(missing)} columns excluded from evaluation due to mismatch (e.g., {list(missing)[:3]}).")

        self.real = real_data[self.common_cols].copy()
        self.syn = synthetic_data[self.common_cols].copy()
        self.r_name = real_name
        self.s_name = synthetic_name
        
        # 2. Type Detection
        self.num_cols = self.real.select_dtypes(include=np.number).columns
        self.cat_cols = self.real.select_dtypes(exclude=np.number).columns
        
        # 3. Preprocessing (Simple Imputation for Evaluation Metrics)
        # Note: Econometric models handle NaNs differently, but distance metrics (DCR, KS) require filled values.
        if len(self.num_cols) > 0:
            self.real[self.num_cols] = self.real[self.num_cols].fillna(self.real[self.num_cols].mean())
            self.syn[self.num_cols] = self.syn[self.num_cols].fillna(self.syn[self.num_cols].mean())
        
        if len(self.cat_cols) > 0:
            for col in self.cat_cols:
                mode_val = self.real[col].mode()[0]
                self.real[col] = self.real[col].fillna(mode_val)
                self.syn[col] = self.syn[col].fillna(mode_val)

class UtilityMetricManager:
    """Orchestrator for Utility/Fidelity Metrics."""
    def __init__(self): self.metrics = []
    def add_metric(self, m): 
        if isinstance(m, list): self.metrics.extend(m)
        else: self.metrics.append(m)
    def evaluate_all(self):
        results = {}
        for metric in self.metrics: results.update(metric.calculate())
        return results

class PrivacyMetricManager:
    """Orchestrator for Privacy/Disclosure Risk Metrics."""
    def __init__(self): self.metrics = []
    def add_metric(self, m):
        if isinstance(m, list): self.metrics.extend(m)
        else: self.metrics.append(m)
    def evaluate_all(self):
        results = {}
        for metric in self.metrics: results.update(metric.calculate())
        return results

# ------------------------------------------------------------------------------
# FIDELITY CALCULATORS
# ------------------------------------------------------------------------------

class BasicStatsCalculator(BaseCalculator):
    """Evaluates the preservation of first moments (Means)."""
    def calculate(self):
        print(f"   -> Calculating Basic Statistics (MAPE)...")
        if len(self.num_cols) == 0: return {}
        
        real_mean = self.real[self.num_cols].mean()
        syn_mean = self.syn[self.num_cols].mean()
        
        # Mean Absolute Percentage Error (MAPE)
        # Added epsilon to avoid division by zero
        mape = np.mean(np.abs((real_mean - syn_mean) / (real_mean + 1e-6)))
        return {f"{self.s_name}_Mean_MAPE": mape}

class KSCalculator(BaseCalculator):
    """
    Kolmogorov-Smirnov (KS) Test.
    Measures the distance between the empirical distribution functions of real and synthetic attributes.
    Score: 1.0 - statistic (Higher is better).
    """
    def calculate(self):
        print(f"   -> Calculating KS Test (Marginal Distributions)...")
        if len(self.num_cols) == 0: return {}
        
        ks_scores = []
        for col in self.num_cols:
            stat, _ = ks_2samp(self.real[col], self.syn[col])
            ks_scores.append(1 - stat) # Invert statistic so 1.0 is perfect overlap
            
        return {f"{self.s_name}_KS_Score_Avg": np.mean(ks_scores)}

class CorrelationCalculator(BaseCalculator):
    """
    Structural Correlation Analysis.
    Computes the Frobenius norm of the difference between correlation matrices.
    """
    def calculate(self):
        print(f"   -> Calculating Structural Correlation...")
        # Encoding categorical variables for correlation matrix
        enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
        r_enc = self.real.copy()
        s_enc = self.syn.copy()
        
        if len(self.cat_cols) > 0:
            # Ensure consistent encoding across both datasets
            full_cat = pd.concat([r_enc[self.cat_cols].astype(str), s_enc[self.cat_cols].astype(str)])
            enc.fit(full_cat)
            r_enc[self.cat_cols] = enc.transform(r_enc[self.cat_cols].astype(str))
            s_enc[self.cat_cols] = enc.transform(s_enc[self.cat_cols].astype(str))

        corr_r = r_enc.corr()
        corr_s = s_enc.corr()
        
        # Frobenius Norm of the difference matrix
        diff_norm = np.linalg.norm(corr_r.fillna(0) - corr_s.fillna(0))
        return {f"{self.s_name}_Correlation_Diff": diff_norm}

# ------------------------------------------------------------------------------
# PRIVACY CALCULATORS
# ------------------------------------------------------------------------------

class DCRCalculator(BaseCalculator):
    """
    Distance to Closest Record (DCR).
    Measures the Euclidean distance between synthetic records and their nearest real neighbor.
    Used to detect overfitting and direct copying.
    """
    def calculate(self):
        print(f"   -> Calculating Privacy (DCR)...")
        if len(self.num_cols) == 0: return {}
        
        # Dimensionality reduction for performance if needed, 
        # or use specific key columns. Here we use all numerics.
        scaler = MinMaxScaler()
        r_scaled = scaler.fit_transform(self.real[self.num_cols])
        s_scaled = scaler.transform(self.syn[self.num_cols])
        
        # Sampling for computational efficiency if N > 3000
        if len(s_scaled) > 3000:
            idx = np.random.choice(len(s_scaled), 3000, replace=False)
            s_scaled = s_scaled[idx]
        
        nbrs = NearestNeighbors(n_neighbors=1).fit(r_scaled)
        distances, _ = nbrs.kneighbors(s_scaled)
        
        return {f"{self.s_name}_DCR_Mean": np.mean(distances)}

class AdversarialAccuracyCalculator(BaseCalculator):
    """
    Adversarial Accuracy (AA).
    Trains a discriminator (Random Forest) to distinguish Real vs. Synthetic data.
    Target AA is 0.5 (indistinguishable). High AA (>0.9) implies distinct artifacts.
    """
    def calculate(self):
        print(f"   -> Calculating Privacy (Adversarial Accuracy)...")
        X_real = self.real.copy()
        X_syn = self.syn.copy()
        X_real['label'] = 0
        X_syn['label'] = 1
        
        combined = pd.concat([X_real, X_syn], axis=0).sample(frac=1.0, random_state=42)
        y = combined['label']
        X = combined.drop('label', axis=1)
        
        # Simple Factorization for Random Forest
        for col in X.select_dtypes(include='object').columns:
            X[col] = pd.factorize(X[col])[0]
        X = X.fillna(0)
        
        # Limit sample size for speed
        if len(X) > 10000:
            X = X.iloc[:10000]
            y = y.iloc[:10000]

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        clf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)
        clf.fit(X_train, y_train)
        
        return {f"{self.s_name}_Adversarial_Accuracy": accuracy_score(y_test, clf.predict(X_test))}

# ==============================================================================
# SECTION B: VISUALIZATION
# ==============================================================================

def plot_feature_distribution(real_df, models_dict, target_col, output_name="Distribution_Comparison.png"):
    """Generates a comparative KDE plot for a specific feature."""
    if target_col not in real_df.columns:
        return

    plt.figure(figsize=(10, 6))
    
    # Plot Real Data
    sns.kdeplot(real_df[target_col], label='Real Data', fill=True, color='black', alpha=0.1, linewidth=2)
    
    # Plot Models
    colors = {'CTGAN': 'blue', 'TVAE': 'green', 'TTVAE': 'red'}
    for name, df in models_dict.items():
        if target_col in df.columns:
            sns.kdeplot(df[target_col], label=f'{name} (Synthetic)', 
                        color=colors.get(name, 'gray'), linestyle='--')
            
    plt.title(f"Distribution Consistency: {target_col}", fontsize=14)
    plt.xlabel(target_col, fontsize=12)
    plt.ylabel("Density", fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_name, dpi=300)
    print(f"   [Graph] Saved distribution plot to '{output_name}'")

# ==============================================================================
# SECTION C: MAIN PIPELINE
# ==============================================================================

def main():
    print("=======================================================")
    print("   FEST FRAMEWORK: SYNTHETIC DATA EVALUATION MODULE    ")
    print("=======================================================")
    
    # 1. Load Ground Truth
    if not os.path.exists('BaseDataset.csv'):
        print("Error: 'BaseDataset.csv' not found.")
        return
    
    real_df = pd.read_csv('BaseDataset.csv')
    # Note: We do NOT drop IDs here to evaluate their preservation as integer sequences.
    print(f"1. Ground Truth Loaded. Shape: {real_df.shape}")

    # 2. Load Synthetic Candidates
    candidate_files = {
        'CTGAN': 'Synthetic_CTGAN.csv',
        'TVAE': 'Synthetic_TVAE.csv',
        'TTVAE': 'Synthetic_TTVAE.csv'
    }
    
    models_data = {}
    for model_name, file_path in candidate_files.items():
        if os.path.exists(file_path):
            print(f"   -> Found Synthetic Data: {model_name}")
            models_data[model_name] = pd.read_csv(file_path)
        else:
            print(f"   -> Warning: {model_name} output not found. Skipping.")

    if not models_data:
        print("No synthetic data found. Please run the generation script first.")
        return

    # 3. Evaluation Loop
    final_results = []

    for model_name, syn_df in models_data.items():
        print(f"\n--- Evaluating Model: {model_name} ---")
        
        # Utility Assessment
        utility_manager = UtilityMetricManager()
        utility_manager.add_metric([
            BasicStatsCalculator(real_df, syn_df, synthetic_name=model_name),
            KSCalculator(real_df, syn_df, synthetic_name=model_name),
            CorrelationCalculator(real_df, syn_df, synthetic_name=model_name)
        ])
        u_res = utility_manager.evaluate_all()
        
        # Privacy Assessment
        privacy_manager = PrivacyMetricManager()
        privacy_manager.add_metric([
            DCRCalculator(real_df, syn_df, synthetic_name=model_name),
            AdversarialAccuracyCalculator(real_df, syn_df, synthetic_name=model_name)
        ])
        p_res = privacy_manager.evaluate_all()
        
        # Consolidate Results
        full_metrics = {**u_res, **p_res}
        
        record = {
            'Model': model_name,
            'KS Score (Fidelity)': full_metrics.get(f"{model_name}_KS_Score_Avg", 0),
            'MAPE (Mean Error)': full_metrics.get(f"{model_name}_Mean_MAPE", 0),
            'Corr. Diff (Structure)': full_metrics.get(f"{model_name}_Correlation_Diff", 999),
            'DCR (Privacy)': full_metrics.get(f"{model_name}_DCR_Mean", 0),
            'Adv. Accuracy': full_metrics.get(f"{model_name}_Adversarial_Accuracy", 0.5)
        }
        final_results.append(record)

    # 4. Generate Final Report
    results_df = pd.DataFrame(final_results)
    
    print("\n=======================================================")
    print("               FINAL COMPARATIVE REPORT                ")
    print("=======================================================")
    # Formatting for better readability in console
    print(results_df.round(4).to_string(index=False))
    
    results_df.to_csv("FEST_Evaluation_Results.csv", index=False)
    print("\n[Output] Full report saved to 'FEST_Evaluation_Results.csv'")
    
    # 5. Determine Winner based on KS Score (Standard for Tabular Fidelity)
    if not results_df.empty:
        winner = results_df.loc[results_df['KS Score (Fidelity)'].idxmax()]
        print(f"\nðŸ† OPTIMAL MODEL: {winner['Model']}")
        print(f"   Reasoning: Highest marginal fidelity (KS = {winner['KS Score (Fidelity)']:.4f})")

    # 6. Generate Visualization
    # Auto-select an interesting numeric column to plot (e.g., Age, Income, or first numeric)
    target_col = 'age' 
    if target_col not in real_df.columns:
        # Fallback to the first numeric column found
        numerics = real_df.select_dtypes(include=np.number).columns
        if len(numerics) > 0:
            target_col = numerics[0]
            
    plot_feature_distribution(real_df, models_data, target_col)

if __name__ == "__main__":
    main()